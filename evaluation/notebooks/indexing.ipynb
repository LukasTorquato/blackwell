{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb4b8c8",
   "metadata": {},
   "source": [
    "#### Warning:\n",
    "To run this must be in the \"blackwell\" package root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31f371",
   "metadata": {},
   "source": [
    "# Medline XML Indexing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading XML from https://medlineplus.gov/xml/mplus_topics_2025-11-29.xml...\n",
      "Downloaded 29957417 bytes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Download the Medline XML file\n",
    "XML_URL = \"https://medlineplus.gov/xml/mplus_topics_2025-11-29.xml\"\n",
    "\n",
    "response = requests.get(XML_URL)\n",
    "xml_content = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757a020",
   "metadata": {},
   "source": [
    "#### Complete XML Topic-Parsing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5abe5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_health_topic(topic_elem, source_url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse a single health-topic XML element into ONE document dictionary.\n",
    "    Matches the exact format of the original database.\n",
    "    \"\"\"\n",
    "    # Extract attributes\n",
    "    topic_id = topic_elem.get('id', '')\n",
    "    title = topic_elem.get('title', '')\n",
    "    url = topic_elem.get('url', '')\n",
    "    date_created = topic_elem.get('date-created', '')\n",
    "    language = topic_elem.get('language', 'English')\n",
    "    meta_desc = topic_elem.get('meta-desc', '')\n",
    "    \n",
    "    # Extract 'also-called' elements\n",
    "    also_called_list = [ac.text.strip() for ac in topic_elem.findall('also-called') if ac.text]\n",
    "    also_called_text = ', '.join(also_called_list)\n",
    "    \n",
    "    # Extract full summary and clean HTML\n",
    "    full_summary = topic_elem.find('full-summary')\n",
    "    full_summary_text = full_summary.text if full_summary is not None and full_summary.text else ''\n",
    "    full_summary_clean = re.sub(r'<[^>]+>', '', full_summary_text)\n",
    "    \n",
    "    # Extract primary institute\n",
    "    primary_inst = topic_elem.find('primary-institute')\n",
    "    primary_inst_text = primary_inst.text.strip() if primary_inst is not None and primary_inst.text else ''\n",
    "    \n",
    "    # Extract see-reference\n",
    "    see_refs = [sr.text.strip() for sr in topic_elem.findall('see-reference') if sr.text]\n",
    "    see_also_text = ', '.join(see_refs)\n",
    "    \n",
    "    # Extract groups (categories)\n",
    "    groups = [g.text.strip() for g in topic_elem.findall('group') if g.text]\n",
    "    categories_text = ', '.join(groups)\n",
    "    \n",
    "    # Extract mesh headings\n",
    "    mesh_headings = [m.find('descriptor').text.strip() \n",
    "                     for m in topic_elem.findall('mesh-heading') \n",
    "                     if m.find('descriptor') is not None and m.find('descriptor').text]\n",
    "    mesh_text = ', '.join(mesh_headings)\n",
    "    \n",
    "    # Extract site links grouped by category\n",
    "    links_by_category = {}\n",
    "    for site in topic_elem.findall('site'):\n",
    "        site_title = site.get('title', '')\n",
    "        site_url = site.get('url', '')\n",
    "        info_cat = site.find('information-category')\n",
    "        category = info_cat.text.strip() if info_cat is not None and info_cat.text else 'General'\n",
    "        \n",
    "        if category not in links_by_category:\n",
    "            links_by_category[category] = []\n",
    "        links_by_category[category].append({'title': site_title, 'url': site_url})\n",
    "    \n",
    "    # Build content\n",
    "    content_parts = []\n",
    "    \n",
    "    if title:\n",
    "        content_parts.append(f\"Title: {title}\")\n",
    "    if meta_desc:\n",
    "        content_parts.append(f\"\\nDescription: {meta_desc}\")\n",
    "    if also_called_text:\n",
    "        content_parts.append(f\"\\nAlso Called: {also_called_text}\")\n",
    "    if full_summary_clean:\n",
    "        content_parts.append(f\"\\nFull Summary:\\n{full_summary_clean}\")\n",
    "    if primary_inst_text:\n",
    "        content_parts.append(f\"\\nPrimary Institute: {primary_inst_text}\")\n",
    "    if see_also_text:\n",
    "        content_parts.append(f\"\\nSee Also: {see_also_text}\")\n",
    "    if categories_text:\n",
    "        content_parts.append(f\"\\nCategories: {categories_text}\")\n",
    "    if mesh_text:\n",
    "        content_parts.append(f\"\\nMedical Subject Headings: {mesh_text}\")\n",
    "    \n",
    "    # Add links grouped by category\n",
    "    if links_by_category:\n",
    "        content_parts.append(\"\\n\\nAdditional Resources:\\n\")\n",
    "        for category, links in links_by_category.items():\n",
    "            content_parts.append(f\"\\n{category}:\\n\")\n",
    "            for link in links:\n",
    "                content_parts.append(f\"  - {link['title']}: {link['url']}\\n\")\n",
    "    \n",
    "    content = \"\".join(content_parts)\n",
    "    \n",
    "    # Build metadata\n",
    "    metadata = {\n",
    "        'source': source_url,\n",
    "        'topic_title': title,\n",
    "        'topic_url': url,\n",
    "        'topic_id': topic_id,\n",
    "        'date_created': date_created,\n",
    "        'num_site_links': sum(len(v) for v in links_by_category.values()),\n",
    "        'language': language,\n",
    "        'site_links': json.dumps([\n",
    "            {'title': l['title'], 'url': l['url'], 'category': cat}\n",
    "            for cat, links in links_by_category.items() for l in links\n",
    "        ]),\n",
    "        'type': 'medlineplus_topic'\n",
    "    }\n",
    "    \n",
    "    return {'content': content, 'metadata': metadata}\n",
    "\n",
    "# Parse XML\n",
    "root = ET.fromstring(xml_content)\n",
    "topics = root.findall('.//health-topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bbbc5",
   "metadata": {},
   "source": [
    "#### Parse all topics into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5c21502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2033 documents from 2033 topics\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for topic in topics:\n",
    "    parsed = parse_health_topic(topic, XML_URL)\n",
    "    doc = Document(page_content=parsed['content'], metadata=parsed['metadata'])\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(documents)} documents from {len(topics)} topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47856eb9",
   "metadata": {},
   "source": [
    "#### Perform chunking over the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebcc0dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10328 chunks from 2033 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1536,\n",
    "    chunk_overlap=256,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23593b",
   "metadata": {},
   "source": [
    "#### Ingest Documents into ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d34c8419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing chunks 0 to 2999\n",
      "Importing chunks 3000 to 5999\n",
      "Importing chunks 3000 to 5999\n",
      "Importing chunks 6000 to 8999\n",
      "Importing chunks 6000 to 8999\n",
      "Importing chunks 9000 to 10327\n",
      "Importing chunks 9000 to 10327\n",
      "Done! Total docs in DB: 10328\n",
      "Done! Total docs in DB: 10328\n"
     ]
    }
   ],
   "source": [
    "# Ingest into ChromaDB\n",
    "from blackwell.config import embeddings_model, DB_PATH, DB_COLLECTION\n",
    "import time\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_collection\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=DB_PATH,\n",
    ")\n",
    "\n",
    "batch_size = 3000\n",
    "\n",
    "# Add in batches of 3000 with 60s delay given rate limits of Tier 1 API\n",
    "if len(chunks) > batch_size:\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size] if i+batch_size < len(chunks) else chunks[i:]\n",
    "        print(f\"Importing chunks {i} to {i + len(batch) - 1}\")\n",
    "        vector_store.add_documents(batch)\n",
    "        time.sleep(60)\n",
    "else:\n",
    "    vector_store.add_documents(chunks)\n",
    "\n",
    "print(f\"Total docs in DB: {len(vector_store.get()['ids'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
